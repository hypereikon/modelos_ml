{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copia de PRiSM SampleRNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c311nixs6iw9"
      },
      "source": [
        "### PRiSM SampleRNN\n",
        "#### Sintesis de sonido neuronal con tensorflow2\n",
        "\n",
        "codigo por Christopher Melen\n",
        "\n",
        "Traducido por @hypereikon"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WjijiJNdGBx",
        "cellView": "form"
      },
      "source": [
        "#@title montar drive\r\n",
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "%cd drive/MyDrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8vVRMk6JZM4",
        "cellView": "form"
      },
      "source": [
        "#@title clonar codigo desde github\r\n",
        "!git clone https://github.com/rncm-prism/prism-samplernn.git\r\n",
        "%cd /content/drive/MyDrive/prism-samplernn/\r\n",
        "!git pull"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Me_60QUL5fdW",
        "cellView": "form"
      },
      "source": [
        "#@title instalar tf2 y librerias necesarias\r\n",
        "%tensorflow_version 2.x\r\n",
        "!pip install librosa\r\n",
        "!pip install natsort\r\n",
        "!pip install pydub\r\n",
        "!pip install soundfile"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gb1xIYy59dbW"
      },
      "source": [
        "### Crear el dataset. si continuas entrenando ignora esta celda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fasHQ6qJQqY7"
      },
      "source": [
        "Para entrenar el modelo necesitamos datos de entrada, en este caso es un solo archivo wav **mono** con suna frecuencia de muestro de **11025**, el cual luego cortaremos en pequeños pedacitos de 5 a 10 segundos.\n",
        "\n",
        "Para entrenar sin problemas en colab se recomienda que el archivo no dure mas de 30 minutos *(se puede más pero tomara más tiempo, con un dataset de 22 min en 20 horas de entrenamiento entrega buenos resultados)*\n",
        "\n",
        "Una vez con el archivo subido, ejecuta la siguiente celda para cortarlo en pequeños pedacitos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qGTkiFNAuGbk",
        "cellView": "form"
      },
      "source": [
        "#@title cortar archivo. si continuas entrenando ignora esta celda\r\n",
        "%mkdir chunks\r\n",
        "!python chunk_audio.py --input_file test.wav --output_dir ./chunks/ --chunk_length 8000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy-cFSHQSpVx"
      },
      "source": [
        "# TensorBoard\n",
        "\n",
        "Tensorflow entrega una herramienta grafica muy util llamada Tensorboard, para monitorear metricas tales como perdida y precision (loss y accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDz-i51aSw8M",
        "cellView": "form"
      },
      "source": [
        "#@title activar tensorboard\r\n",
        "%load_ext tensorboard\r\n",
        "%tensorboard --logdir logdir/default/train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-XD1cLW-PRk"
      },
      "source": [
        "#Entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9RrIVRj9FCb"
      },
      "source": [
        "#### Parametros de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUoFBAGT8uVX"
      },
      "source": [
        "Una vez ya teniendo el dataset en pequeños pedacitos podemos entrenar. La siguiente tabla explica los hiperparametros de entrenamiento, que se pueden agregar a la siguiente celda de entrenamiento, si no entiendes bien es mejor no cambiarlos.\n",
        "\n",
        "| Parameter Name             | Description           | Default Value  | Required?   |\n",
        "| ---------------------------|-----------------------|----------------| -----------|\n",
        "| `id`                     | Id for the training session.          | `default`           | No        |\n",
        "| `data_dir`               | Path to the directory containing the training data.           | `None`           | Yes        |\n",
        "| `verbose`                | Set training output verbosity. If `False` training step output is overwritten, if `True` (the default) it is written to a new line.           | `None`           | No        |\n",
        "| `logdir_root`            | Location in which to store training log files and checkpoints. All such files are placed in a subdirectory with the id of the training session.           | `./logdir`           | No      |\n",
        "| `output_dir`             | Path to the directory for audio generated during training.           | `./generated`           | No      |\n",
        "| `config_file`            | File containing the configuration parameters for the training model. Note that this file must contain valid JSON, and should have a name that conforms to the `*.config.json` pattern. | `./default.config.json`         | No        |\n",
        "| `num_epochs`             | Number of epochs to run the training. | 100           | No        |\n",
        "| `batch_size`             | Size of the mini-batch. It is recommended that the batch size divide the length of the training corpus without remainder, otherwise the dataset will be truncated to the nearest multiple of the batch size. | 64         | No        |\n",
        "| `optimizer`              | TensorFlow optimizer to use for training. (`adam`, `sgd` or `rmsprop`) | `adam`        | No        |\n",
        "| `learning_rate`          | Learning rate of the training optimizer.   | 0.001         | No        |\n",
        "| `reduce_learning_rate_after`          | Exponentially reduce learning rate after this many epochs.   | `None`         | No        |\n",
        "| `momentum`               | Momentum of the training optimizer (applies to `sgd` and `rmsprop` only).   | 0.9      | No        |\n",
        "| `checkpoint_every`       | Interval (in epochs) at which to generate a checkpoint file. Defaults to 1, for every epoch.   | 1      | No        |\n",
        "| `checkpoint_policy`      | Policy for saving checkpoints - `Always` to save at the epoch interval determined by the value of `checkpoint_every`, or `Best` to save only when the loss and accuracy have improved since the last save.   | `All`      | No        |\n",
        "| `max_checkpoints`        | Maximum number of checkpoints to keep on disk during training. Defaults to 5. Pass `None` to keep all checkpoints.   | 5      | No        |\n",
        "| `resume`                 | Whether to resume training, either from the last available checkpoint or from one supplied using the `resume_from` parameter.   | `True`      | No        |\n",
        "| `resume_from`            | Checkpoint from which to resume training. Ignored when `resume` is `False`.   | `None`      | No        |\n",
        "| `early_stopping_patience`| Number of epochs with no improvement after which training will be stopped.   | 3      | No        |\n",
        "| `generate`               | Whether to generate audio output during training. Generation is aligned with checkpoints, meaning that audio is only generated after a new checkpoint has been created.   | `True`      | No        |\n",
        "| `max_generate_per_epoch` | Maximum number of output files to generate at the end of each epoch.   | 1      | No        |\n",
        "| `sample_rate`            | Sample rate of the generated audio. | 22050         | No        |\n",
        "| `output_file_dur`        | Duration of generated audio files (in seconds). | 3         | No        |\n",
        "| `temperature`            | Sampling temperature for generated audio. | 0.75         | No        |\n",
        "| `seed`                   | Path to audio for seeding when generating audio. | `None`         | No        |\n",
        "| `seed_offset`            | Starting offset of the seed audio. | 0         | No        |\n",
        "| `num_val_batches`               | Number of batches to reserve for validation. | 1         | No        |\n",
        "\n",
        "\n",
        "\n",
        "Model parameters are specified through a JSON configuration file, which may be passed to the training script through the `--config_file` parameter (defaults to the supplied `default.config.json`). The following table lists the available model parameters (note that all parameters are optional and have defaults):\n",
        "\n",
        "| Parameter Name           | Description           | Default Value  |\n",
        "| -------------------------|-----------------------|----------------|\n",
        "| `seq_len`                | RNN sequence length. Note that the value must be evenly-divisible by the top tier frame size.        | 1024           |\n",
        "| `frame_sizes`            | Frame sizes (in samples) of the two upper tiers in the architecture, in ascending order. Note that the frame size of the upper tier must be an even multiple of that of the lower tier.  | [16,64]            |\n",
        "| `dim`                    | RNN hidden layer dimensionality          | 1024         | \n",
        "| `rnn_type`         | RNN type to use, either `gru` or `lstm`           | `gru`           |\n",
        "| `num_rnn_layers`         | Depth of the RNN in each of the two upper tiers           | 4          |\n",
        "| `q_type`                 | Quantization type (`mu-law` or `linear`)          | `mu-law`          |\n",
        "| `q_levels`               | Number of quantization channels (note that if `q_type` is `mu-law` this parameter is ignored, as mu-law quantization requires 256 channels)     | 256           |\n",
        "| `emb_size`               | Size of the embedding layer in the bottom tier (sample-level MLP)         | 256          |\n",
        "| `skip_conn`               | Whether to add skip connections to the model's RNN layers         | `False`          |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scosW3k4AF1T"
      },
      "source": [
        "####entrenar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGlEVyQgDjWS"
      },
      "source": [
        "si continuas entrenando un modelo ya entrenado debes tener `--resume True` en la siguiente celda.\r\n",
        "\r\n",
        "\r\n",
        "`--batch size ` debe ser un divisor de el total de pedacitos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDqV1DP453bj"
      },
      "source": [
        "!python train.py \\\n",
        "  --data_dir ./chunks \\\n",
        "  --resume True \\\n",
        "  --num_epochs 200 \\\n",
        "  --batch_size 34 \\\n",
        "  --max_checkpoints 100 \\\n",
        "  --checkpoint_every 5 \\\n",
        "  --output_file_dur 5 \\\n",
        "  --sample_rate 11025"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unk2itY9_cnd"
      },
      "source": [
        "#Generar audio, inferir, evaluar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs8gTtshAats"
      },
      "source": [
        "####Parametros de generacion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTBf04xG884j"
      },
      "source": [
        "Con un modelo ya entrenado podemos usar el script `generate.py`  para generar audio. Los parametros del script son los siguientes:\n",
        "\n",
        "| Parameter Name             | Description           | Default Value  | Required?   |\n",
        "| ---------------------------|-----------------------|----------------| -----------|\n",
        "| `output_path`              | Path to the generated .wav file.          | `None`           | Yes        |\n",
        "| `checkpoint_path`          | Path to a saved checkpoint for the model.  (xxx.cpkt-xx         | `None`           | Yes        |\n",
        "| `config_file`              | Path to the JSON config for the model.          | `None`           | Yes        |\n",
        "| `dur`                      | Duration of generated audio.           | 3           | No       |\n",
        "| `num_seqs`                 | Number of audio sequences to generate.          | 1           | No        |\n",
        "| `sample_rate`              | Sample rate of the generated audio.          | 44100           | No        |\n",
        "| `temperature`              | Sampling temperature for generated audio. Multiple values may be passed, to match the number of sequences to be generated.  | 0.75         | No        |\n",
        "| `seed`                     | Path to audio for seeding when generating audio. | `None`         | No        |\n",
        "| `seed_offset`              | Starting offset of the seed audio. | 0         | No        |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oa8SkOmBAtIK"
      },
      "source": [
        "####generar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kopNLOKA0jG"
      },
      "source": [
        "debes actualizar el `--checkpoint_path` al ultimo checkpoint del modelo entrenado.\r\n",
        "\r\n",
        "`--seed_offset` indica de que cuantos samples a partir del `seed` genera. \r\n",
        "Por ejemplo, si ya genere 15 segundos con un seed, puedo generar la continuacion de este sonido. Esto calculando el total de frames generados `(15*11025=165375)` y seteando con este el `seed_offset` "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY17YXXK5_zP"
      },
      "source": [
        "!python generate.py \\\n",
        "  --output_path ./generated/default/test.wav \\\n",
        "  --checkpoint_path  ./logdir/default/10.02.2021_01.16.00/model.ckpt-99 \\\n",
        "  --sample_rate 11025 \\\n",
        "  --temperature 0.998 \\\n",
        "  --dur 15 \\\n",
        "  --seed 99 \\\n",
        "  --seed_offset 0 \\\n",
        "  --config_file ./default.config.json"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}