{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VQGANxCLIP ESP",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hypereikon/modelos_ml/blob/main/VQGANxCLIP_ESP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgZLlaBOiPKK"
      },
      "source": [
        "#VQGANxCLIP\n",
        "\n",
        "sintesis de imagen, texto-a-imagen\n",
        "\n",
        "####codigo por [@aicrumb](https://github.com/aicrumb)\n",
        "traducido por [@hypereikon](https://www.instagram.com/hypereikon/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-bdfxia3n2X"
      },
      "source": [
        "###configuración"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSfISAhyPmyp",
        "cellView": "form"
      },
      "source": [
        "#@title Instalar las dependencias necesarias, solo es necesario una vez por sesión\n",
        "!git clone https://github.com/openai/CLIP\n",
        "!git clone https://github.com/CompVis/taming-transformers\n",
        "!pip install ftfy regex tqdm omegaconf pytorch-lightning\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_1024.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/8088892a516d4e3baf92/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_1024.ckpt\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fconfigs%2Fmodel.yaml&dl=1' > vqgan_imagenet_f16_16384.yaml\n",
        "!curl -L 'https://heibox.uni-heidelberg.de/d/a7530b09fed84f80a887/files/?p=%2Fckpts%2Flast.ckpt&dl=1' > vqgan_imagenet_f16_16384.ckpt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkmxzSFfbywA"
      },
      "source": [
        "###generación\n",
        "\n",
        "Argumentos:\n",
        "\n",
        "* `model`: el \"diccionario\" que usará. entregan distintos resultados, recomiendo probar cada uno con la misma frases.\n",
        "* `input_url`: link a la imagen inicial, opcional.\n",
        "* `input_img`: ruta de colab de imagen inicial, opcional.\n",
        "* `seed`: cambiara el resultado, recomiendo jugar cambiandola con la misma frase.\n",
        "* `macro`: frase que buscara el modelo. puedes mezclar frases con coma; \"frase1,frase2\".\n",
        "* `micro`: \"detalles\" adicionales a la frase.\n",
        "* `width`: ancho en pixeles (maximo 700px, a menos pixeles mayor *coherencia*).\n",
        "* `height`: alto en pixeles (maximo 700px, a menos pixeles mayor *coherencia*).\n",
        "* `penalize`: frases a restar, si aparecen conceptos que no quieres puedes eliminarlos con esto. puedes activarlo o desactivarlo en `penalize_text`.\n",
        "* `step_size`: que tanto avanzara entre *frames*, a un mayor valor mas *rapido avanzará*.\n",
        "* `steps`:  cantidad total de *frames* que llegará, a un mayor valor mayor cantidad de detalles.\n",
        "\n",
        "Si no usaras input_img ni uploaded_img debes dejarlas en blanco.\n",
        "\n",
        "####Este modelo es multilingüe, puedes buscar en ingles, español, etc. o mezclados. En ingles entrega mejores resultados ٩(˘◡˘)۶\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdlpRFL8UAlW",
        "cellView": "form"
      },
      "source": [
        "import argparse\n",
        "import math\n",
        "from pathlib import Path\n",
        "import sys\n",
        "model = \"16384\" #@param [\"16384\", \"1024\"] {type:\"string\"}\n",
        "sys.path.append('./taming-transformers')\n",
        "\n",
        "from IPython import display\n",
        "from omegaconf import OmegaConf\n",
        "from PIL import Image\n",
        "from taming.models import cond_transformer, vqgan\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from CLIP import clip\n",
        "\n",
        "def sinc(x):\n",
        "    return torch.where(x != 0, torch.sin(math.pi * x) / (math.pi * x), x.new_ones([]))\n",
        "\n",
        "\n",
        "def lanczos(x, a):\n",
        "    cond = torch.logical_and(-a < x, x < a)\n",
        "    out = torch.where(cond, sinc(x) * sinc(x/a), x.new_zeros([]))\n",
        "    return out / out.sum()\n",
        "\n",
        "\n",
        "def ramp(ratio, width):\n",
        "    n = math.ceil(width / ratio + 1)\n",
        "    out = torch.empty([n])\n",
        "    cur = 0\n",
        "    for i in range(out.shape[0]):\n",
        "        out[i] = cur\n",
        "        cur += ratio\n",
        "    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n",
        "\n",
        "\n",
        "def resample(input, size, align_corners=True):\n",
        "    n, c, h, w = input.shape\n",
        "    dh, dw = size\n",
        "\n",
        "    input = input.view([n * c, 1, h, w])\n",
        "\n",
        "    if dh < h:\n",
        "        kernel_h = lanczos(ramp(dh / h, 2), 2).to(input.device, input.dtype)\n",
        "        pad_h = (kernel_h.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n",
        "        input = F.conv2d(input, kernel_h[None, None, :, None])\n",
        "\n",
        "    if dw < w:\n",
        "        kernel_w = lanczos(ramp(dw / w, 2), 2).to(input.device, input.dtype)\n",
        "        pad_w = (kernel_w.shape[0] - 1) // 2\n",
        "        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n",
        "        input = F.conv2d(input, kernel_w[None, None, None, :])\n",
        "\n",
        "    input = input.view([n, c, h, w])\n",
        "    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)\n",
        "    \n",
        "\n",
        "def replace_grad(fake, real):\n",
        "    return fake.detach() - real.detach() + real\n",
        "\n",
        "\n",
        "class Prompt(nn.Module):\n",
        "    def __init__(self, embed, weight=1., stop=float('-inf')):\n",
        "        super().__init__()\n",
        "        self.register_buffer('embed', embed)\n",
        "        self.register_buffer('weight', torch.as_tensor(weight))\n",
        "        self.register_buffer('stop', torch.as_tensor(stop))\n",
        "\n",
        "    def forward(self, input):\n",
        "        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n",
        "        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n",
        "        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n",
        "        dists = dists * self.weight.sign()\n",
        "        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n",
        "\n",
        "\n",
        "def parse_prompt(prompt):\n",
        "    vals = prompt.rsplit(':', 2)\n",
        "    vals = vals + ['', '1', '-inf'][len(vals):]\n",
        "    return vals[0], float(vals[1]), float(vals[2])\n",
        "\n",
        "\n",
        "class MakeCutouts(nn.Module):\n",
        "    def __init__(self, cut_size, cutn, cut_pow=1.):\n",
        "        super().__init__()\n",
        "        self.cut_size = cut_size\n",
        "        self.cutn = cutn\n",
        "        self.cut_pow = cut_pow\n",
        "\n",
        "    def forward(self, input):\n",
        "        sideY, sideX = input.shape[2:4]\n",
        "        max_size = min(sideX, sideY)\n",
        "        min_size = min(sideX, sideY, self.cut_size)\n",
        "        cutouts = []\n",
        "        for _ in range(self.cutn):\n",
        "            size = int(torch.rand([])**self.cut_pow * (max_size - min_size) + min_size)\n",
        "            offsetx = torch.randint(0, sideX - size + 1, ())\n",
        "            offsety = torch.randint(0, sideY - size + 1, ())\n",
        "            cutout = input[:, :, offsety:offsety + size, offsetx:offsetx + size]\n",
        "            cutouts.append(resample(cutout, (self.cut_size, self.cut_size)).clamp(0, 1))\n",
        "        return torch.cat(cutouts, dim=0)\n",
        "\n",
        "\n",
        "def load_vqgan_model(config_path, checkpoint_path):\n",
        "    config = OmegaConf.load(config_path)\n",
        "    if config.model.target == 'taming.models.vqgan.VQModel':\n",
        "        model = vqgan.VQModel(**config.model.params)\n",
        "        model.eval().requires_grad_(False)\n",
        "        model.init_from_ckpt(checkpoint_path)\n",
        "    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n",
        "        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n",
        "        parent_model.eval().requires_grad_(False)\n",
        "        parent_model.init_from_ckpt(checkpoint_path)\n",
        "        model = parent_model.first_stage_model\n",
        "    else:\n",
        "        raise ValueError(f'unknown model type: {config.model.target}')\n",
        "    del model.loss\n",
        "    return model\n",
        "\n",
        "input_url = \"\"#@param {\"type\":\"string\"}\n",
        "if(input_url!=\"\"):\n",
        "    !wget -i \"$input_url\" -O \"/content/input_img.jpg\" -q\n",
        "    input_image = \"/content/input_img.jpg\"\n",
        "else:\n",
        "    input_image = None\n",
        "uploaded_img = \"\"#@param {\"type\":\"string\"}\n",
        "if(uploaded_img!=\"\"):\n",
        "    input_image = uploaded_img\n",
        "else:\n",
        "    input_image = None\n",
        "seed =  0#@param\n",
        "macro = \"a banana doing standup\" #@param {\"type\":\"string\"}\n",
        "micro = \"comedy\" #@param {\"type\":\"string\"}\n",
        "width = 256 #@param\n",
        "height =  256#@param\n",
        "step_size = 0.06 #@param\n",
        "args = argparse.Namespace(\n",
        "    prompts=[macro, macro, micro],\n",
        "    size=[width, height],\n",
        "    init_image=input_image,\n",
        "    init_weight=0.,\n",
        "    clip_model='ViT-B/32',\n",
        "    vqgan_config='vqgan_imagenet_f16_{}.yaml'.format(model),\n",
        "    vqgan_checkpoint='vqgan_imagenet_f16_{}.ckpt'.format(model),\n",
        "    step_size=step_size,\n",
        "    cutn=64,\n",
        "    cut_pow=1.,\n",
        "    display_freq=10,\n",
        "    seed=seed\n",
        ")\n",
        "\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('Using device:', device)\n",
        "\n",
        "model = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\n",
        "perceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)\n",
        "\n",
        "cut_size = perceptor.visual.input_resolution\n",
        "e_dim = model.quantize.e_dim\n",
        "make_cutouts = MakeCutouts(cut_size, args.cutn, cut_pow=args.cut_pow)\n",
        "n_toks = model.quantize.n_e\n",
        "toksX, toksY = args.size[0] // 16, args.size[1] // 16\n",
        "\n",
        "torch.manual_seed(args.seed)\n",
        "\n",
        "if args.init_image:\n",
        "    pil_image = Image.open(args.init_image).convert('RGB')\n",
        "    pil_image = pil_image.resize((toksX * 16, toksY * 16), Image.LANCZOS)\n",
        "    z, *_ = model.encode(TF.to_tensor(pil_image).to(device).unsqueeze(0) * 2 - 1)\n",
        "else:\n",
        "    one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n",
        "    z = one_hot.matmul(model.quantize.embedding.weight)\n",
        "    z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2)\n",
        "z_orig = z.clone()\n",
        "z.requires_grad_(True)\n",
        "opt = optim.Adam([z], lr=args.step_size)\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n",
        "                                 std=[0.26862954, 0.26130258, 0.27577711])\n",
        "\n",
        "pMs = []\n",
        "\n",
        "for prompt in args.prompts:\n",
        "    txt, weight, stop = parse_prompt(prompt)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, weight, stop).to(device))\n",
        "\n",
        "penalize_text = True #@param {\"type\":\"boolean\"}\n",
        "penalize = \"graffiti, text,sad,confused\" #@param {\"type\":\"string\"}\n",
        "\n",
        "if(penalize_text):\n",
        "    txt, weight, stop = parse_prompt(penalize)\n",
        "    embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n",
        "    pMs.append(Prompt(embed, -weight, stop).to(device))\n",
        "\n",
        "\n",
        "def synth(z):\n",
        "    z_q, *_ = model.quantize(z)\n",
        "    return model.decode(z_q).add(1).div(2).clamp(0, 1)\n",
        "!mkdir frames\n",
        "@torch.no_grad()\n",
        "def checkin(i, losses):\n",
        "    losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n",
        "    tqdm.write(f'Step: {i} | Avg: {sum(losses).item():g} | Losses: {losses_str}')\n",
        "    out = synth(z)\n",
        "    a = TF.to_pil_image(out[0].cpu())\n",
        "    a.save('progress.png')\n",
        "    a.save('frames/{}.png'.format(str(int(i/10)).rjust(4, \"0\")))\n",
        "    display.display(display.Image('progress.png'))\n",
        "\n",
        "def ascend_txt():\n",
        "    out = synth(z)\n",
        "    iii = perceptor.encode_image(normalize(make_cutouts(out+(torch.randn(out.shape)*0.1).cuda()))).float()\n",
        "\n",
        "    result = []\n",
        "\n",
        "    if args.init_weight:\n",
        "        result.append(F.mse_loss(z, z_orig) * args.init_weight / 2)\n",
        "\n",
        "    for prompt in pMs:\n",
        "        result.append(prompt(iii))\n",
        "\n",
        "    return result\n",
        "\n",
        "def train(i):\n",
        "    opt.zero_grad()\n",
        "    lossAll = ascend_txt()\n",
        "    if i % args.display_freq == 0:\n",
        "        checkin(i, lossAll)\n",
        "    loss = sum(lossAll)\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "steps = 1000#@param\n",
        "i = 0\n",
        "try:\n",
        "    with tqdm() as pbar:\n",
        "        while i<steps:\n",
        "            train(i)\n",
        "            i += 1\n",
        "            pbar.update()\n",
        "except KeyboardInterrupt:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdRnPgwc7qVR"
      },
      "source": [
        "###Crear video y eliminar imagenes, para generar denuevo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ty-eddypDbdP",
        "cellView": "form"
      },
      "source": [
        "#@title crear video\n",
        "!ffmpeg -i \"/content/frames/%04d.png\" /content/video.mp4\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "mp4 = open('/content/video.mp4','rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-6b2DFoI2gS",
        "cellView": "form"
      },
      "source": [
        "#@title borrar frames para generar denuevo\n",
        "%cd /content/frames/\n",
        "!rm *.png\n",
        "%cd /content"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLWZD7YfMuoP"
      },
      "source": [
        "No usar este notebook para fines comerciales (NFTs). (◔◡◔)\n",
        "\n",
        "Si te parece util esta herramient puedes apoyar a Chloe comprando sus [NFTs](https://www.hicetnunc.xyz/tz/tz1Ss4GU4bmhZKPxWYCLWJAzaLeySuBveY1N)"
      ]
    }
  ]
}
